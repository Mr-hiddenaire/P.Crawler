import logging
from config import base as base_config
import time
from services.content import ContentService
from utils.selenium.chrome import browser
from pyquery import PyQuery
from utils.tool import hash_with_blake2b

def do_original_source_crawler_with_selenium(url=None):
    url_hash = hash_with_blake2b(url)
    is_scraped = ContentService.is_page_scraped(url_hash, base_config.IS_ASIA)

    if is_scraped is False:
        logging.info('url:' + url + ' >>>>>> hash:' + url_hash + ' is scraping')

        """ driver initialization """
        driver = browser.get_driver()
        driver.get(url)

        """ sleep 6 sec to wait for cf protection finish """
        time.sleep(6)

        html = driver.page_source
        driver.close()

        return html
    else:
        logging.info('url:' + url + ' >>>>>> hash:' + url_hash + ' is scraped')

def get_html_generator_according_to_original_html(original_html=None):
    doc = PyQuery(original_html)

    html_generator = doc('.video').items()

    return html_generator

def parse_data_according_to_html_generator(html_generator=None, base_url=None):
    result = []
    for html in html_generator:
        doc = PyQuery(html)
        name = doc('.title').text()
        unique_id = doc('.id').text()
        """ Value assignation later """
        tags = ''
        types = base_config.IS_ASIA
        """ No need it anymore. thumbnail will be generated by ffmpeg """
        thumb_url = ''
        """ Value assignation later """
        torrent_url = ''
        """ Value assignation later """
        torrent_path = ''
        entry_point=base_url
        detail_url = base_config.CRAWLER_URL_ASIA + doc('a').attr('href').replace('./', '')
        pick_up_status = 0
        pick_up_time = 0
        is_archive = 0
        archive_priority = 0
        list_url_hash = hash_with_blake2b(base_url)
        detail_url_hash = hash_with_blake2b(detail_url)
        is_scraped = 0

        info = {
            'name': name,
            'unique_id': unique_id,
            'tags': tags,
            'types': types,
            'thumb_url': thumb_url,
            'torrent_url': torrent_url,
            'torrent_path': torrent_path,
            'detail_url': detail_url,
            'entry_point': entry_point,
            'pick_up_status': pick_up_status,
            'pick_up_time': pick_up_time,
            'is_archive': is_archive,
            'archive_priority': archive_priority,
            'list_url_hash': list_url_hash,
            'detail_url_hash': detail_url_hash,
            'is_scraped': is_scraped,

        }

        result.append(info)

    return result